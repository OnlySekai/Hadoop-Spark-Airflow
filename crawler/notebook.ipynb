{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web3 import Web3\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Web3Util:\n",
    "    def __init__(self, provider):\n",
    "        self.web3_instance = Web3(provider)\n",
    "\n",
    "    def get_block(self, block_number):\n",
    "        try:\n",
    "            block = self.web3_instance.eth.get_block(block_number, True)\n",
    "            return block\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Could not get information of getBlock\") from e\n",
    "web3Util = Web3Util(Web3.HTTPProvider('https://eth.llamarpc.com'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load db.json\n",
    "with open('db.json') as f:\n",
    "    db = json.load(f)\n",
    "last_block = db.get('last_block', 0)\n",
    "block_step = db.get('block_step', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    blocks = list(executor.map(web3Util.get_block, range(last_block, last_block+ block_step)))\n",
    "db['last_block'] = last_block + block_step\n",
    "#save db.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx =[]\n",
    "for block in blocks:\n",
    "    for transaction in block.transactions:\n",
    "        trx.append(dict(transaction))\n",
    "for transaction in trx:\n",
    "    if \"accessList\" in transaction:\n",
    "        del transaction[\"accessList\"]\n",
    "    transaction['blockHash'] = transaction['blockHash'].hex()\n",
    "    transaction['hash'] = transaction['hash'].hex()\n",
    "    transaction['input'] = transaction['input'].hex()\n",
    "    transaction['r'] = transaction['r'].hex()\n",
    "    transaction['s'] = transaction['s'].hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'transaction-{last_block}.csv'\n",
    "import csv\n",
    "if len(trx):\n",
    "    with open(f'../hadoop_namenode/{filename}', 'w', newline='') as csvfile:\n",
    "        fieldnames = list(trx[0].keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for transaction in trx:\n",
    "            writer.writerow(transaction)\n",
    "#save db.json\n",
    "with open('db.json', 'w') as f:\n",
    "    json.dump(db, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 17:20:43,508 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['docker', 'exec', '2a7e7322b681990ff358eb6b5e2d9c51e10bbb89d0110ff4774800ac0057bd4a', 'hdfs', 'dfs', '-put', '-f', '/hadoop/dfs/name/transaction-18925040.csv', '/'], returncode=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "container_id = '2a7e7322b681990ff358eb6b5e2d9c51e10bbb89d0110ff4774800ac0057bd4a'\n",
    "command = f'docker exec {container_id} hdfs dfs -put -f /hadoop/dfs/name/{filename} /'\n",
    "subprocess.run(command.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|                from|          fee|\n",
      "+--------------------+-------------+\n",
      "|0xae2Fc483527B8EF...|3766611205311|\n",
      "|0x80C1969588bD9a0...|3035136662465|\n",
      "|0x75e89d5979E4f6F...|1899832305883|\n",
      "|0x0D0707963952f2f...| 982647695730|\n",
      "|0x16D5783a96ab20c...| 672470959598|\n",
      "+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('app').getOrCreate()\n",
    "\n",
    "df = spark.read.csv('hdfs://localhost:9000/transaction*.csv', header=True, inferSchema=True).cache()\n",
    "\n",
    "df_fee = df.groupBy('from').sum('gasPrice').withColumnRenamed('sum(gasPrice)', 'fee').orderBy('fee', ascending=False)\n",
    "df_fee.show(10)\n",
    "df_fee.repartition(1).write.csv('hdfs://localhost:9000/fee.csv', header=True)\n",
    "df = spark.read.csv('hdfs://localhost:9000/fee.csv', header=True, inferSchema=True).cache()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
